{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "471a3335-4595-41b5-a377-fe17d4b926bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = ''\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f031e7ba-0e29-461f-b551-b605205e9f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "parent_folder = 'nli_dataset'\n",
    "language_codes = [name for name in os.listdir(parent_folder)\n",
    "                if os.path.isdir(os.path.join(parent_folder, name))]\n",
    "language_codes.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0cb4ecf2-056e-4dfa-b95e-a07cf6bc5312",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_code_to_name = {\n",
    "    'amh': 'Amharic',\n",
    "    'ara': 'Arabic',\n",
    "    'asm': 'Assamese',\n",
    "    'aym': 'Aymara',\n",
    "    'ben': 'Bengali',\n",
    "    'bul': 'Bulgarian',\n",
    "    'bzd': 'Bribri',\n",
    "    'cat': 'Catalan',\n",
    "    'cni': 'Asháninka',\n",
    "    'deu': 'German',\n",
    "    'ell': 'Greek',\n",
    "    'eng': 'English',\n",
    "    'ewe': 'Ewe',\n",
    "    'fra': 'French',\n",
    "    'grn': 'Guarani',\n",
    "    'guj': 'Gujarati',\n",
    "    'hau': 'Hausa',\n",
    "    'hch': 'Wixarika',\n",
    "    'hin': 'Hindi',\n",
    "    'ibo': 'Igbo',\n",
    "    'ind': 'Indonesian',\n",
    "    'jpn': 'Japanese',\n",
    "    'kan': 'Kannada',\n",
    "    'kin': 'Kinyarwanda',\n",
    "    'kor': 'Korean',\n",
    "    'lin': 'Lingala',\n",
    "    'lug': 'Luganda',\n",
    "    'mal': 'Malayalam',\n",
    "    'mar': 'Marathi',\n",
    "    'mya': 'Burmese',\n",
    "    'nah': 'Nahuatl',\n",
    "    'ori': 'Odia (Oriya)',\n",
    "    'orm': 'Oromo',\n",
    "    'oto': 'Otomi',\n",
    "    'pan': 'Punjabi',\n",
    "    'pat': 'Jamaican Patois',\n",
    "    'pol': 'Polish',\n",
    "    'por': 'Portuguese',\n",
    "    'quy': 'Quechua',\n",
    "    'ron': 'Romanian',\n",
    "    'rus': 'Russian',\n",
    "    'shp': 'Shipibo-Conibo',\n",
    "    'sna': 'chiShona',\n",
    "    'sot': 'Sesotho',\n",
    "    'spa': 'Spanish',\n",
    "    'swa': 'Swahili',\n",
    "    'tam': 'Tamil',\n",
    "    'tar': 'Rarámuri',\n",
    "    'tel': 'Telugu',\n",
    "    'tha': 'Thai',\n",
    "    'tur': 'Turkish',\n",
    "    'twi': 'Twi',\n",
    "    'urd': 'Urdu',\n",
    "    'vie': 'Vietnamese',\n",
    "    'wol': 'Wolof',\n",
    "    'xho': 'isiXhosa',\n",
    "    'yor': 'Yoruba',\n",
    "    'zho': 'Chinese',\n",
    "    'zul': 'isiZulu'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d15ed787-2335-4249-8793-ed117ba66ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Given the following premise and hypothesis in {{language}}, identify if the premise entails, contradicts, or is neutral towards the hypothesis. Please respond with exact 'entailment', 'contradiction', or 'neutral'. \\n\\nPremise: {{premise}} \\nHypothesis: {{hypothesis}}\"\n",
    "prompt_nli = \"Given the following premise and hypothesis in {}, identify if the premise entails, contradicts, or is neutral towards the hypothesis. Please respond with exact 'entailment', 'contradiction', or 'neutral'. \\n\\nPremise: {} \\nHypothesis: {}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e67724dd-7a4b-4f4b-827c-590f32b48922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_label(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ])\n",
    "    reply = completion.choices[0].message.content\n",
    "    reply = reply.lower()\n",
    "\n",
    "    label = ''\n",
    "    if 'entail' in reply:\n",
    "        label = 0\n",
    "    elif 'neutral' in reply:\n",
    "        label = 1\n",
    "    elif 'contradict' in reply:\n",
    "        label = 2\n",
    "    else:\n",
    "        label = -1\n",
    "    return reply, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed795787-949b-4fca-8bc8-8f6d3f996185",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_to_run = language_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc4d3390-fc44-485b-91a1-1953fdcae0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zul\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 600/600 [04:45<00:00,  2.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "labels = {}\n",
    "gpt_replies = {}\n",
    "result_accuracies = {}\n",
    "\n",
    "for language_code in languages_to_run:\n",
    "    if language_code == 'eng':\n",
    "        continue\n",
    "    print(language_code)\n",
    "    labels[language_code]=[]\n",
    "    gpt_replies[language_code]=[]\n",
    "    result_accuracies[language_code] = []\n",
    "    accurate = 0\n",
    "    \n",
    "    df = pd.read_csv(\"nli_dataset/{}/test.csv\".format(language_code))\n",
    "    \n",
    "    for i in tqdm(range(600)): #length of devtest\n",
    "        premise = df.iloc[i]['premise']\n",
    "        hypothesis = df.iloc[i]['hypothesis']\n",
    "        gold_label = df.iloc[i]['label']\n",
    "        \n",
    "        prompt = prompt_nli.format(lang_code_to_name[language_code],premise,hypothesis)\n",
    "        gpt_reply, label = get_label(prompt)\n",
    "        gpt_replies[language_code].append(gpt_reply)\n",
    "        labels[language_code].append(label)\n",
    "        if label == gold_label:\n",
    "            accurate+=1\n",
    "            \n",
    "    result_accuracies[language_code] = accurate\n",
    "    \n",
    "    result_df = pd.DataFrame({\n",
    "    \"premise\": df['premise'],\n",
    "    \"hypothesis\": df['hypothesis'],\n",
    "    \"gpt_label\": labels[language_code]\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    result_df.to_csv(\"nli_predicted_labels_gpt4o/{}.csv\".format(language_code), index=False)\n",
    "\n",
    "    df_gpt_replies = pd.DataFrame({\n",
    "        \"premise\": df['premise'],\n",
    "        \"hypothesis\": df['hypothesis'],\n",
    "        \"gpt_reply\": gpt_replies[language_code]\n",
    "    })\n",
    "\n",
    "    df_gpt_replies.to_csv(\"nli_replies_gpt4o/{}.csv\".format(language_code), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d13f148d-f805-41da-906f-99329ea58626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eng': 530}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "117f9b19-3e81-4537-9a96-b20f26520f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save to a JSON file\n",
    "with open(\"result_accuracies_nli_gpt.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result_accuracies, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Language name\": [lang_code_to_name[language_code] for language_code in language_codes],\n",
    "    \"Language code\": language_codes,\n",
    "    \"Accuracy\": [round(result_accuracies[language_code]*100/600, 1) for language_code in language_codes]\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"nli_results_gpt4o_april2025.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32f321f6-371f-4cae-b99e-3305eba938ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_lang_codes = {\n",
    "    'XNLI': [\n",
    "        'eng', 'fra', 'spa', 'deu', 'ell', 'bul', 'rus', 'tur',\n",
    "        'ara', 'vie', 'tha', 'zho', 'hin', 'swa', 'urd'\n",
    "    ],\n",
    "    'AfriXNLI': [\n",
    "        # West Africa\n",
    "        'ewe', 'hau', 'ibo', 'twi', 'wol', 'yor',\n",
    "        # East Africa\n",
    "        'amh', 'kin', 'lug', 'swa', 'orm',\n",
    "        # Southern Africa\n",
    "        'sna', 'xho', 'zul', 'sot',\n",
    "        # Central Africa\n",
    "        'lin'\n",
    "    ],\n",
    "    'IndicXNLI': [\n",
    "        'asm', 'guj', 'kan', 'mal', 'mar', 'ori', 'pan',\n",
    "        'tam', 'tel', 'hin', 'ben'\n",
    "    ],\n",
    "    'AmericasXNLI': [\n",
    "        'aym', 'cni', 'bzd', 'grn', 'nah', 'oto', 'quy', 'tar', 'shp', 'hch'\n",
    "    ],\n",
    "    'XNLI-ca': ['cat'],\n",
    "    'myXNLI': ['mya'],\n",
    "    'IndoNLI': ['ind'],\n",
    "    'JNLI': ['jpn'],\n",
    "    'Portugese': ['por'],\n",
    "    'Polish': ['pol'],\n",
    "    'JamPatoisNLI': ['pat'],\n",
    "    'Korean': ['kor'],\n",
    "    'Romainian': ['ron']\n",
    "}\n",
    "#create dataset to average performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6d7ed6b-ba2f-4160-8720-8839d220a0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Dataset  Average Accuracy\n",
      "0           XNLI              77.9\n",
      "1       AfriXNLI              61.8\n",
      "2      IndicXNLI              72.7\n",
      "3   AmericasXNLI              45.0\n",
      "4        XNLI-ca              82.8\n",
      "5         myXNLI              72.8\n",
      "6        IndoNLI              87.2\n",
      "7           JNLI              81.3\n",
      "8      Portugese              96.8\n",
      "9         Polish              83.5\n",
      "10  JamPatoisNLI              90.0\n",
      "11        Korean              86.2\n",
      "12     Romainian              64.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df, lang_code_to_name, result_accuracies, language_codes, and dataset_to_lang_codes are already defined\n",
    "\n",
    "# Create a mapping from language code to accuracy for fast lookup\n",
    "lang_code_to_accuracy = dict(zip(df[\"Language code\"], df[\"Accuracy\"]))\n",
    "\n",
    "# Compute average accuracy per dataset\n",
    "dataset_avg_accuracies = {}\n",
    "\n",
    "for dataset, lang_codes in dataset_to_lang_codes.items():\n",
    "    # Filter to only those lang codes that are in df\n",
    "    valid_langs = [code for code in lang_codes if code in lang_code_to_accuracy]\n",
    "    \n",
    "    if valid_langs:\n",
    "        scores = [lang_code_to_accuracy[code] for code in valid_langs]\n",
    "        avg_accuracy = round(sum(scores) / len(scores), 1)\n",
    "        dataset_avg_accuracies[dataset] = avg_accuracy\n",
    "\n",
    "# Convert to a DataFrame if desired\n",
    "dataset_avg_df = pd.DataFrame.from_dict(dataset_avg_accuracies, orient='index', columns=['Average Accuracy']).reset_index()\n",
    "dataset_avg_df = dataset_avg_df.rename(columns={'index': 'Dataset'})\n",
    "\n",
    "print(dataset_avg_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "07bb5af6-fc8a-4099-b6f5-fb4a5a49083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_avg_df.to_csv('nli_results_gpt_4o_april2025_by_family.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2308e5f6-64d2-4f5d-893f-6ab30a03508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df2 = pd.read_csv('overall_results_nli.csv')\n",
    "z=df2.groupby('language_family') \\\n",
    "       .agg({'language_name':'size','Accuracy gpt-4o (April 2025)':'mean','Accuracy Gemini-2-Flash (April 2025)':'mean','Accuracy Gemma3-27B':'mean'} ) \\\n",
    "       .rename(columns={'language_name':'count'}) \\\n",
    "       .sort_values(by=['count'], ascending=False) \\\n",
    "       .reset_index().round(1)\n",
    "z.to_csv('overall_by_family_nli.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f66ecf5-168b-41b8-8031-1f35e3e24266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb15ad-025e-4fb8-a019-0db25670d2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma3-env",
   "language": "python",
   "name": "gemma3-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
