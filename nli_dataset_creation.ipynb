{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e78b48c-5913-41b4-b199-fea5202de143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seacrowd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f79ab0e-b090-405b-abe4-4d781accbd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adaf8446438c4603885dc0f57ee723ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-2614419e00195781.parquet:   0%|          | 0.00/130M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f427a43c56e5411b94a998c1a3d1cde9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-9c168eb31d1d810b.parquet:   0%|          | 0.00/443k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d9463a86f04ebc80018b04163006b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-0fd9f93baf8c9cdb.parquet:   0%|          | 0.00/881k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0592faeb1094bbf8a8e63d7bd64b311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42da5e0f2afa4313b181c3bf91afdbd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ef9d37136e492bbbddddc1b8c8e200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dset = load_dataset(\"SEACrowd/myxnli\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd7153d2-ad57-4010-ba79-fe01c3e2afa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['genre', 'label', 'sentence1_en', 'sentence2_en', 'sentence1_my', 'sentence2_my'],\n",
       "    num_rows: 5010\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "324d2fa6-4c3d-4e9f-a980-144a916b6f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae68a53769e4e25a3f69632611864c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063c5c9341fd420d83f453ab352f6475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dev.tsv:   0%|          | 0.00/73.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d939b1b82994c5e993b89ed2bf4af45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.tsv:   0%|          | 0.00/93.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e8eb651191b450ca127541207cf5fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6c9f1112924642aeb157e82b91c4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_xnli_eng = load_dataset(\"masakhane/afrixnli\", \"eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c17fcf2-5c98-47cb-8b6c-26048e6fc284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'The constitutional text of 1787 had stipulated a right of slave owners to recover slaves who had escaped into free territory.',\n",
       " 'hypothesis': 'Part of the text of the constitution was written in 1787.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_xnli_eng['validation'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9057f9e-8635-4932-a298-10700bb818f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a84619472584a79be14443bf357adfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/20.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17225ff3ad954cba941f303b13b3e9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/50.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e571aabff8d14216a18e52f1c112eaf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/308k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c193f6260a03425a9012f89467c0d682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/157k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b5bf928d934b7183b29a30439a890d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/392702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec65c33afaaa4b169cac2fe26edaabae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5010 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e975dfade74b86af304c182da2cc2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_xnli = load_dataset(\"facebook/xnli\", \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b24d93c0-4aca-4959-aef5-d10ba0741655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'But uh, think about it.',\n",
       " 'hypothesis': \"It's nothing you would want to think about.\",\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_xnli['validation'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c7e5cc2-af3b-47d4-ae87-89a790ab67b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take first of each category in dev set. \n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the main dataset folder\n",
    "dataset_path = \"nli_dataset\"\n",
    "ds_xnli_swa = load_dataset(\"masakhane/afrixnli\", \"swa\")\n",
    "x=ds_xnli_swa['validation']\n",
    "\n",
    "# Initialize lists for selected rows\n",
    "premise, hypothesis, label = [], [], []\n",
    "\n",
    "# Extract specific rows\n",
    "for i in range(10):\n",
    "    for j in range(3):\n",
    "        index = i * 45 + j\n",
    "        if index < len(x):  # Ensure index is within bounds\n",
    "            premise.append(x[index]['premise'])\n",
    "            hypothesis.append(x[index]['hypothesis'])\n",
    "            label.append(x[index]['label'])\n",
    "\n",
    "# Create a new DataFrame with selected rows\n",
    "new_df = pd.DataFrame({\n",
    "    \"premise\": premise,\n",
    "    \"hypothesis\": hypothesis,\n",
    "    \"label\": label\n",
    "})\n",
    "\n",
    "# Save back to the same dev.csv file (overwriting it)\n",
    "new_df.to_csv('nli_dataset/swa/dev.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca75a993-cfa1-4611-b3e6-99840bf28927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: nli_dataset/eng/dev.csv\n",
      "Updated nli_dataset/eng/dev.csv\n",
      "Processing: nli_dataset/amh/dev.csv\n",
      "Updated nli_dataset/amh/dev.csv\n",
      "Processing: nli_dataset/ibo/dev.csv\n",
      "Updated nli_dataset/ibo/dev.csv\n",
      "Processing: nli_dataset/sna/dev.csv\n",
      "Updated nli_dataset/sna/dev.csv\n",
      "Processing: nli_dataset/lin/dev.csv\n",
      "Updated nli_dataset/lin/dev.csv\n",
      "Processing: nli_dataset/wol/dev.csv\n",
      "Updated nli_dataset/wol/dev.csv\n",
      "Processing: nli_dataset/ewe/dev.csv\n",
      "Updated nli_dataset/ewe/dev.csv\n",
      "Processing: nli_dataset/lug/dev.csv\n",
      "Updated nli_dataset/lug/dev.csv\n",
      "Processing: nli_dataset/xho/dev.csv\n",
      "Updated nli_dataset/xho/dev.csv\n",
      "Processing: nli_dataset/kin/dev.csv\n",
      "Updated nli_dataset/kin/dev.csv\n",
      "Processing: nli_dataset/twi/dev.csv\n",
      "Updated nli_dataset/twi/dev.csv\n",
      "Processing: nli_dataset/zul/dev.csv\n",
      "Updated nli_dataset/zul/dev.csv\n",
      "Processing: nli_dataset/orm/dev.csv\n",
      "Updated nli_dataset/orm/dev.csv\n",
      "Processing: nli_dataset/yor/dev.csv\n",
      "Updated nli_dataset/yor/dev.csv\n",
      "Processing: nli_dataset/hau/dev.csv\n",
      "Updated nli_dataset/hau/dev.csv\n",
      "Processing: nli_dataset/sot/dev.csv\n",
      "Updated nli_dataset/sot/dev.csv\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "#take first of each category in dev set. \n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the main dataset folder\n",
    "dataset_path = \"nli_dataset\"\n",
    "\n",
    "# Iterate through all subfolders inside nli_dataset\n",
    "for subfolder in os.listdir(dataset_path):\n",
    "    subfolder_path = os.path.join(dataset_path, subfolder)\n",
    "    dev_csv_path = os.path.join(subfolder_path, \"dev.csv\")\n",
    "\n",
    "    # Ensure the subfolder is a directory and contains dev.csv\n",
    "    if os.path.isdir(subfolder_path) and os.path.exists(dev_csv_path):\n",
    "        print(f\"Processing: {dev_csv_path}\")\n",
    "\n",
    "        # Read the dev.csv file\n",
    "        x = pd.read_csv(dev_csv_path)\n",
    "\n",
    "        # Initialize lists for selected rows\n",
    "        premise, hypothesis, label = [], [], []\n",
    "\n",
    "        # Extract specific rows\n",
    "        for i in range(10):\n",
    "            for j in range(3):\n",
    "                index = i * 45 + j\n",
    "                if index < len(x):  # Ensure index is within bounds\n",
    "                    premise.append(x.iloc[index]['premise'])\n",
    "                    hypothesis.append(x.iloc[index]['hypothesis'])\n",
    "                    label.append(x.iloc[index]['label'])\n",
    "\n",
    "        # Create a new DataFrame with selected rows\n",
    "        new_df = pd.DataFrame({\n",
    "            \"premise\": premise,\n",
    "            \"hypothesis\": hypothesis,\n",
    "            \"label\": label\n",
    "        })\n",
    "\n",
    "        # Save back to the same dev.csv file (overwriting it)\n",
    "        new_df.to_csv(dev_csv_path, index=False)\n",
    "        print(f\"Updated {dev_csv_path}\")\n",
    "\n",
    "print(\"Processing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96758c06-e0a0-42da-9eee-435b35e920d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa539f94aa654d259045ef9bd5f648fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00004.parquet:   0%|          | 0.00/238M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00dc39c17146455ea8b96620455ee5f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00004.parquet:   0%|          | 0.00/239M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6c054905c0400699aaf47c301558a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00002-of-00004.parquet:   0%|          | 0.00/238M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac93f4a789845dd80f91e8effade6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00003-of-00004.parquet:   0%|          | 0.00/239M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cedb54e3af464bb4ae17307ef6300ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/6.77M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fda521fc9434928966cedf13e1e96a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/3.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914e2e3994c240a38cfdf7c7776449f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/392702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0686ffba530f42e486f694d8ae8f0d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5010 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f2828aae484d2da1aceb40de54d31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xnli_all = load_dataset(\"facebook/xnli\", \"all_languages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3befc0e1-6f9d-4c41-afcc-b413139cd223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': \"Well, I wasn't even thinking about that, but I was so frustrated, and, I ended up talking to him again.\",\n",
       " 'hypothesis': 'I havent spoken to him again.',\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_xnli['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f1a8f0a-d347-44a2-823f-59fb840cf354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 6, 9, 15, 24, 27, 30, 33, 36, 42, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 501, 504, 507, 510, 513, 519, 525, 528, 531, 534, 537, 540, 543, 546, 549, 552, 555, 558, 561, 564, 1002, 1005, 1008, 1011, 1014, 1017, 1020, 1026, 1032, 1035, 1038, 1041, 1044, 1047, 1053, 1059, 1068, 1077, 1080, 1086, 1503, 1506, 1509, 1515, 1518, 1521, 1524, 1527, 1533, 1554, 1560, 1563, 1569, 1572, 1575, 1578, 1581, 1587, 1593, 1602, 2004, 2013, 2019, 2022, 2028, 2040, 2043, 2049, 2052, 2055, 2061, 2064, 2070, 2073, 2091, 2103, 2112, 2115, 2124, 2139, 2505, 2508, 2511, 2523, 2526, 2532, 2535, 2541, 2544, 2547, 2550, 2553, 2562, 2565, 2568, 2571, 2577, 2586, 2589, 2601, 3006, 3009, 3012, 3015, 3018, 3021, 3027, 3030, 3036, 3039, 3042, 3045, 3048, 3051, 3057, 3060, 3063, 3066, 3072, 3078, 3507, 3510, 3513, 3516, 3519, 3522, 3528, 3531, 3534, 3537, 3540, 3546, 3549, 3552, 3555, 3564, 3570, 3573, 3576, 3579, 4008, 4011, 4014, 4017, 4020, 4029, 4032, 4035, 4038, 4041, 4047, 4056, 4059, 4062, 4065, 4068, 4071, 4074, 4077, 4086, 4512, 4515, 4518, 4527, 4530, 4533, 4536, 4545, 4554, 4557, 4563, 4566, 4569, 4572, 4578, 4581, 4587, 4593, 4614, -1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load your test.csv\n",
    "df = pd.read_csv(\"nli_dataset/eng/test.csv\")\n",
    "\n",
    "# Load XNLI dataset (replace 'en' with your language if needed)\n",
    "xnli = load_dataset(\"facebook/xnli\", \"en\")\n",
    "xnli_premises = xnli[\"test\"][\"premise\"]\n",
    "\n",
    "# Collect indices of matches\n",
    "matching_indices_test = []\n",
    "\n",
    "for i in range(0, len(df), 3):\n",
    "    test_premise = df.iloc[i][\"premise\"]\n",
    "    try:\n",
    "        index = xnli_premises.index(test_premise)  # Finds first match\n",
    "    except ValueError:\n",
    "        index = -1  # If not found\n",
    "    matching_indices_test.append(index)\n",
    "\n",
    "print(matching_indices_test)\n",
    "#last one is 4617"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9c0b68e5-dcf5-4cf5-bd17-c2ca039bf38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved nli_dataset/en/test.csv\n",
      "Saved nli_dataset/fr/test.csv\n",
      "Saved nli_dataset/es/test.csv\n",
      "Saved nli_dataset/de/test.csv\n",
      "Saved nli_dataset/el/test.csv\n",
      "Saved nli_dataset/bg/test.csv\n",
      "Saved nli_dataset/ru/test.csv\n",
      "Saved nli_dataset/tr/test.csv\n",
      "Saved nli_dataset/ar/test.csv\n",
      "Saved nli_dataset/vi/test.csv\n",
      "Saved nli_dataset/th/test.csv\n",
      "Saved nli_dataset/zh/test.csv\n",
      "Saved nli_dataset/hi/test.csv\n",
      "Saved nli_dataset/sw/test.csv\n",
      "Saved nli_dataset/ur/test.csv\n"
     ]
    }
   ],
   "source": [
    "#I need to figure out correct index of the entries in xnli \n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "languages = [\"en\", \"fr\", \"es\", \"de\", \"el\", \"bg\", \"ru\", \"tr\", \"ar\", \"vi\", \"th\", \"zh\", \"hi\", \"sw\", \"ur\"]\n",
    "\n",
    "base_dir = \"nli_dataset\"\n",
    "\n",
    "for lang in languages:\n",
    "    # Load the XNLI dataset for the given language\n",
    "    dataset = load_dataset(\"facebook/xnli\", lang)\n",
    "    \n",
    "    # Get the test set\n",
    "    test_data = dataset[\"test\"]\n",
    "    \n",
    "    premise = []\n",
    "    hypothesis = []\n",
    "    label = []\n",
    "\n",
    "    for index in matching_indices:\n",
    "        for j in range(3):\n",
    "            premise.append(test_data[index+j][\"premise\"])\n",
    "            hypothesis.append(test_data[index+j][\"hypothesis\"])\n",
    "            label.append(test_data[index+j][\"label\"])\n",
    "\n",
    "    # Creating DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"premise\": premise,\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "    # Define language-specific directory\n",
    "    lang_dir = os.path.join(base_dir, lang)\n",
    "    os.makedirs(lang_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    # Save to CSV\n",
    "    file_path = os.path.join(lang_dir, \"test.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "039c6658-bed2-4af8-8c99-1b253a2e24ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 249, 501, 747, 996, 1245, 1494, 1743, 1995, 2241]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8a50d6fb-642a-42d1-85a3-c3d2515ba116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 6, 9, 15, 24, 27, 30, 33, 36, 42, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 501, 504, 507, 510, 513, 519, 525, 528, 531, 534, 537, 540, 543, 546, 549, 552, 555, 558, 561, 564, 1002, 1005, 1008, 1011, 1014, 1017, 1020, 1026, 1032, 1035, 1038, 1041, 1044, 1047, 1053, 1059, 1068, 1077, 1080, 1086, 1503, 1506, 1509, 1515, 1518, 1521, 1524, 1527, 1533, 1554, 1560, 1563, 1569, 1572, 1575, 1578, 1581, 1587, 1593, 1602, 2004, 2013, 2019, 2022, 2028, 2040, 2043, 2049, 2052, 2055, 2061, 2064, 2070, 2073, 2091, 2103, 2112, 2115, 2124, 2139, 2505, 2508, 2511, 2523, 2526, 2532, 2535, 2541, 2544, 2547, 2550, 2553, 2562, 2565, 2568, 2571, 2577, 2586, 2589, 2601, 3006, 3009, 3012, 3015, 3018, 3021, 3027, 3030, 3036, 3039, 3042, 3045, 3048, 3051, 3057, 3060, 3063, 3066, 3072, 3078, 3507, 3510, 3513, 3516, 3519, 3522, 3528, 3531, 3534, 3537, 3540, 3546, 3549, 3552, 3555, 3564, 3570, 3573, 3576, 3579, 4008, 4011, 4014, 4017, 4020, 4029, 4032, 4035, 4038, 4041, 4047, 4056, 4059, 4062, 4065, 4068, 4071, 4074, 4077, 4086, 4512, 4515, 4518, 4527, 4530, 4533, 4536, 4545, 4554, 4557, 4563, 4566, 4569, 4572, 4578, 4581, 4587, 4593, 4614, 4617]\n"
     ]
    }
   ],
   "source": [
    "print(matching_indices_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "87e59815-2813-41d6-aa3c-4c8cf060c212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved nli_dataset/my/dev.csv\n"
     ]
    }
   ],
   "source": [
    "#I need to figure out correct index of the entries in xnli \n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "languages = [\"ca\"]\n",
    "\n",
    "base_dir = \"nli_dataset\"\n",
    "\n",
    "for lang in languages:\n",
    "    # Load the XNLI dataset for the given language\n",
    "    dataset = load_dataset(\"projecte-aina/xnli-ca\")\n",
    "    \n",
    "    # Get the test set\n",
    "    test_data = dataset[\"test\"]\n",
    "    \n",
    "    # Extracting 60 continuous entries after every 501 rows\n",
    "    premise = []\n",
    "    hypothesis = []\n",
    "    label = []\n",
    "\n",
    "    for index in matching_indices_test:\n",
    "        for j in range(3):\n",
    "            premise.append(test_data[index+j][\"premise\"])\n",
    "            hypothesis.append(test_data[index+j][\"hypothesis\"])\n",
    "            label.append(test_data[index+j][\"label\"])\n",
    "\n",
    "    # Creating DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"premise\": premise,\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "    # Define language-specific directory\n",
    "    lang_dir = os.path.join(base_dir, lang)\n",
    "    os.makedirs(lang_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    # Save to CSV\n",
    "    file_path = os.path.join(lang_dir, \"test.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "43abcb1d-6f49-41b2-a168-c540f3372af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved nli_dataset/ca/dev.csv\n"
     ]
    }
   ],
   "source": [
    "#I need to figure out correct index of the entries in xnli \n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "languages = [\"as\", \"gu\", \"kn\", \"ml\", \"mr\", \"or\", \"pa\", \"ta\", \"te\", \"hi\", \"bn\"]\n",
    "\n",
    "base_dir = \"nli_dataset\"\n",
    "for lang in languages:\n",
    "    # Load the XNLI dataset for the given language\n",
    "    dataset = load_dataset(\"Divyanshu/indicxnli\", lang)\n",
    "    \n",
    "    # Get the test set\n",
    "    test_data = dataset[\"validation\"]\n",
    "    \n",
    "    # Extracting 60 continuous entries after every 501 rows\n",
    "    premise = []\n",
    "    hypothesis = []\n",
    "    label = []\n",
    "\n",
    "    for index in matching_indices:\n",
    "        for j in range(3):\n",
    "            premise.append(test_data[index+j][\"premise\"])\n",
    "            hypothesis.append(test_data[index+j][\"hypothesis\"])\n",
    "            label.append(test_data[index+j][\"label\"])\n",
    "\n",
    "    # Creating DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"premise\": premise,\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "    # Define language-specific directory\n",
    "    lang_dir = os.path.join(base_dir, lang)\n",
    "    os.makedirs(lang_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    # Save to CSV\n",
    "    file_path = os.path.join(lang_dir, \"dev.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8efc0c40-6e3d-4fd5-9632-56cde40954d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved nli_dataset/fr/dev.csv\n"
     ]
    }
   ],
   "source": [
    "#I need to figure out correct index of the entries in xnli \n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "languages = [\"fr\"]\n",
    "\n",
    "base_dir = \"nli_dataset\"\n",
    "matching_indices = [0, 249, 501, 747, 996, 1245, 1494, 1743, 1995, 2241]\n",
    "\n",
    "for lang in languages:\n",
    "    # Load the XNLI dataset for the given language\n",
    "    dataset = load_dataset(\"facebook/xnli\", lang)\n",
    "    \n",
    "    # Get the test set\n",
    "    test_data = dataset[\"validation\"]\n",
    "    \n",
    "    # Extracting 60 continuous entries after every 501 rows\n",
    "    premise = []\n",
    "    hypothesis = []\n",
    "    label = []\n",
    "\n",
    "    for index in matching_indices:\n",
    "        for j in range(3):\n",
    "            premise.append(test_data[index+j][\"premise\"])\n",
    "            hypothesis.append(test_data[index+j][\"hypothesis\"])\n",
    "            label.append(test_data[index+j][\"label\"])\n",
    "\n",
    "    # Creating DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"premise\": premise,\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "    # Define language-specific directory\n",
    "    lang_dir = os.path.join(base_dir, lang)\n",
    "    os.makedirs(lang_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    # Save to CSV\n",
    "    file_path = os.path.join(lang_dir, \"dev.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fde11f11-bafa-4284-919a-59371501b8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genre': 'facetoface',\n",
       " 'label': 0,\n",
       " 'sentence1_en': \"Well, I wasn't even thinking about that, but I was so frustrated, and, I ended up talking to him again.\",\n",
       " 'sentence2_en': 'I havent spoken to him again.',\n",
       " 'sentence1_my': 'ငါက ဒါတွေကိုတောင် စဥ်းစားနေခဲ့တာမဟုတ်ပေမယ့် ငါတော်တော်စိတ်ညစ်နေခဲ့ပြီး ငါသူနဲ့စကားပြန်ပြောဖြစ်ခဲ့တယ်။',\n",
       " 'sentence2_my': 'ငါ သူ့ကို စကား ထပ်မပြောဖြစ်ဘူး။'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b6bada6f-da82-4767-848f-df9c574cdfb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 5010\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 2490\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_xnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ea059a46-1528-4a44-a1ff-548d7bc4e079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved nli_dataset/aym/dev.csv\n",
      "Saved nli_dataset/bzd/dev.csv\n",
      "Saved nli_dataset/cni/dev.csv\n",
      "Saved nli_dataset/gn/dev.csv\n",
      "Saved nli_dataset/hch/dev.csv\n",
      "Saved nli_dataset/nah/dev.csv\n",
      "Saved nli_dataset/oto/dev.csv\n",
      "Saved nli_dataset/quy/dev.csv\n",
      "Saved nli_dataset/shp/dev.csv\n",
      "Saved nli_dataset/tar/dev.csv\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"nala-cub/americas_nli\", \"aym\")\n",
    "\n",
    "#I need to figure out correct index of the entries in xnli \n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "languages = [\"aym\", \"bzd\", \"cni\", \"gn\", \"hch\", \"nah\", \"oto\", \"quy\", \"shp\", \"tar\"]\n",
    "\n",
    "base_dir = \"nli_dataset\"\n",
    "\n",
    "for lang in languages:\n",
    "    # Load the XNLI dataset for the given language\n",
    "    dataset = load_dataset(\"nala-cub/americas_nli\", lang)\n",
    "    \n",
    "    # Get the test set\n",
    "    test_data = dataset[\"validation\"]\n",
    "    \n",
    "    # Extracting 60 continuous entries after every 501 rows\n",
    "    premise = []\n",
    "    hypothesis = []\n",
    "    label = []\n",
    "\n",
    "    for index in range(30):\n",
    "        premise.append(test_data[index][\"premise\"])\n",
    "        hypothesis.append(test_data[index][\"hypothesis\"])\n",
    "        label.append(test_data[index][\"label\"])\n",
    "\n",
    "    # Creating DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"premise\": premise,\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "    # Define language-specific directory\n",
    "    lang_dir = os.path.join(base_dir, lang)\n",
    "    os.makedirs(lang_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    # Save to CSV\n",
    "    file_path = os.path.join(lang_dir, \"dev.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6f4beed-ecb3-4879-8634-f11d459fb2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987330613b7a4fcfb5a1fed1b1b3e10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bcdcea18a743bc980863ea791e38a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "indonli.py:   0%|          | 0.00/5.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f221bd5fbb42dfbe2d8c2b66bfa78a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.79M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b40d3ad4b04608bb1581c28f8ff0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/785k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "950b14502f9640338150d777f323a261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/796k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dcc4c9583794b3b936919b4849e4cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.61M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25cc447419384b159ae54589a0016c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10330 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe5571e7f5e4ab9b718acc18ab5042b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2197 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93780a9c1660413aaa44e1e31a472781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_lay split:   0%|          | 0/2201 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd03cc32c15412984fb8d4b4bf2eecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_expert split:   0%|          | 0/2984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "indo_ds = load_dataset('afaji/indonli', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a95be983-0263-4b61-8c3e-4586ce1a305a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 10330\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 2197\n",
       "    })\n",
       "    test_lay: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 2201\n",
       "    })\n",
       "    test_expert: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 2984\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indo_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cc52172-0508-4d70-bb16-f98b5c2f65de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved nli_dataset/ind/test.csv\n"
     ]
    }
   ],
   "source": [
    "#I need to figure out correct index of the entries in xnli \n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "languages = [\"ind\"]\n",
    "\n",
    "base_dir = \"nli_dataset\"\n",
    "\n",
    "for lang in languages:\n",
    "    \n",
    "    # Get the test set\n",
    "    test_data = indo_ds[\"test_expert\"]\n",
    "    \n",
    "    # Extracting 60 continuous entries after every 501 rows\n",
    "    premise = []\n",
    "    hypothesis = []\n",
    "    label = []\n",
    "    num_entail, num_neutral, num_contradiction = 0,0,0\n",
    "    index = 0\n",
    "    \n",
    "    while (num_entail<200 or num_neutral<200 or num_contradiction<200):\n",
    "        lbl = test_data[index][\"label\"]\n",
    "        if lbl == 0 and num_entail<200:\n",
    "            premise.append(test_data[index][\"premise\"])\n",
    "            hypothesis.append(test_data[index][\"hypothesis\"])\n",
    "            label.append(lbl)\n",
    "            num_entail += 1\n",
    "        if lbl == 1 and num_neutral<200:\n",
    "            premise.append(test_data[index][\"premise\"])\n",
    "            hypothesis.append(test_data[index][\"hypothesis\"])\n",
    "            label.append(lbl)\n",
    "            num_neutral += 1\n",
    "        if lbl == 2 and num_contradiction<200:\n",
    "            premise.append(test_data[index][\"premise\"])\n",
    "            hypothesis.append(test_data[index][\"hypothesis\"])\n",
    "            label.append(lbl)\n",
    "            num_contradiction += 1\n",
    "        index +=1\n",
    " \n",
    "    # Creating DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"premise\": premise,\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "    # Define language-specific directory\n",
    "    lang_dir = os.path.join(base_dir, lang)\n",
    "    os.makedirs(lang_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    # Save to CSV\n",
    "    file_path = os.path.join(lang_dir, \"test.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51f1342d-9677-49b7-a7c7-c4729bef237d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved nli_dataset/por/test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "languages = [\"por\"]\n",
    "\n",
    "base_dir = \"nli_dataset\"\n",
    "\n",
    "for lang in languages:\n",
    "    \n",
    "    # Get the test set\n",
    "    test_data = load_dataset(\"hapaxlegomenon/InferBR\")[\"test\"]\n",
    "    \n",
    "    premise = []\n",
    "    hypothesis = []\n",
    "    label = []\n",
    "    num_entail, num_neutral, num_contradiction = 0,0,0\n",
    "    index = 0\n",
    "    \n",
    "    while (num_entail<200 or num_neutral<200 or num_contradiction<200):\n",
    "        lbl = test_data[index][\"label\"]\n",
    "        if lbl == 0 and num_entail<200:\n",
    "            premise.append(test_data[index][\"premise\"])\n",
    "            hypothesis.append(test_data[index][\"hypothesis\"])\n",
    "            label.append(2)\n",
    "            num_entail += 1\n",
    "        if lbl == 1 and num_neutral<200:\n",
    "            premise.append(test_data[index][\"premise\"])\n",
    "            hypothesis.append(test_data[index][\"hypothesis\"])\n",
    "            label.append(0)\n",
    "            num_neutral += 1\n",
    "        if lbl == 2 and num_contradiction<200:\n",
    "            premise.append(test_data[index][\"premise\"])\n",
    "            hypothesis.append(test_data[index][\"hypothesis\"])\n",
    "            label.append(1)\n",
    "            num_contradiction += 1\n",
    "        index +=1\n",
    " \n",
    "    # Creating DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"premise\": premise,\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "    # Define language-specific directory\n",
    "    lang_dir = os.path.join(base_dir, lang)\n",
    "    os.makedirs(lang_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    # Save to CSV\n",
    "    file_path = os.path.join(lang_dir, \"test.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0492465-3ca1-4534-b932-37bc86cfd17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved nli_dataset/por/dev.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "languages = [\"por\"]\n",
    "\n",
    "base_dir = \"nli_dataset\"\n",
    "\n",
    "for lang in languages:\n",
    "    \n",
    "    # Get the test set\n",
    "    test_data = load_dataset(\"hapaxlegomenon/InferBR\")[\"validation\"]\n",
    "    \n",
    "    premise = []\n",
    "    hypothesis = []\n",
    "    label = []\n",
    "    num_entail, num_neutral, num_contradiction = 0,0,0\n",
    "    index = 0\n",
    "    \n",
    "    while (num_entail<10 or num_neutral<10 or num_contradiction<10):\n",
    "        lbl = test_data[index][\"label\"]\n",
    "        if lbl == 0 and num_entail<10:\n",
    "            premise.append(test_data[index][\"premise\"])\n",
    "            hypothesis.append(test_data[index][\"hypothesis\"])\n",
    "            label.append(2)\n",
    "            num_entail += 1\n",
    "        if lbl == 1 and num_neutral<10:\n",
    "            premise.append(test_data[index][\"premise\"])\n",
    "            hypothesis.append(test_data[index][\"hypothesis\"])\n",
    "            label.append(0)\n",
    "            num_neutral += 1\n",
    "        if lbl == 2 and num_contradiction<10:\n",
    "            premise.append(test_data[index][\"premise\"])\n",
    "            hypothesis.append(test_data[index][\"hypothesis\"])\n",
    "            label.append(1)\n",
    "            num_contradiction += 1\n",
    "        index +=1\n",
    " \n",
    "    # Creating DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"premise\": premise,\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "    # Define language-specific directory\n",
    "    lang_dir = os.path.join(base_dir, lang)\n",
    "    os.makedirs(lang_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    # Save to CSV\n",
    "    file_path = os.path.join(lang_dir, \"dev.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "085f7187-ae0e-4053-a9be-18abbf98c05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ce2d1f63f343efbf1e085c5721b499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.89k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b256e3195a24d9c8ac9885137d24e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.jsonl:   0%|          | 0.00/928k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a9d96f8fb84e8d81c24b7a468c3d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation.jsonl:   0%|          | 0.00/105k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4453bf0fe9f34970b922ed15c2446e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.jsonl:   0%|          | 0.00/1.02M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045629d809fe47499cb10d4a591f0817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/4439 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a455c0afdc446b83dc546970ee333b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/495 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e708b6850c848afb76c71abad6e1fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4906 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "ds = load_dataset(\"sdadas/sick_pl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f8c1988-cefa-4629-a8d0-0a9e1888806b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pair_ID': '6',\n",
       " 'sentence_A': 'Nie ma chłopca bawiącego się na świeżym powietrzu i nie ma uśmiechniętego mężczyzny',\n",
       " 'sentence_B': 'Grupa dzieci bawi się na podwórku , a w tle stoi stary mężczyzna',\n",
       " 'relatedness_score': 3.299999952316284,\n",
       " 'entailment_judgment': 'NEUTRAL'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73fd6f00-7549-449c-b3d7-a303355a29bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved nli_dataset/pol/test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "languages = [\"pol\"]\n",
    "\n",
    "base_dir = \"nli_dataset\"\n",
    "\n",
    "for lang in languages:\n",
    "    \n",
    "    # Get the test set\n",
    "    test_data = load_dataset(\"sdadas/sick_pl\")[\"test\"]\n",
    "    \n",
    "    premise = []\n",
    "    hypothesis = []\n",
    "    label = []\n",
    "    num_entail, num_neutral, num_contradiction = 0,0,0\n",
    "    index = 0\n",
    "    \n",
    "    while (num_entail<200 or num_neutral<200 or num_contradiction<200):\n",
    "        lbl = test_data[index][\"entailment_judgment\"]\n",
    "        if lbl == 'ENTAILMENT' and num_entail<200:\n",
    "            premise.append(test_data[index][\"sentence_A\"])\n",
    "            hypothesis.append(test_data[index][\"sentence_B\"])\n",
    "            label.append(0)\n",
    "            num_entail += 1\n",
    "        if lbl == 'NEUTRAL' and num_neutral<200:\n",
    "            premise.append(test_data[index][\"sentence_A\"])\n",
    "            hypothesis.append(test_data[index][\"sentence_B\"])\n",
    "            label.append(1)\n",
    "            num_neutral += 1\n",
    "        if lbl == 'CONTRADICTION' and num_contradiction<200:\n",
    "            premise.append(test_data[index][\"sentence_A\"])\n",
    "            hypothesis.append(test_data[index][\"sentence_B\"])\n",
    "            label.append(2)\n",
    "            num_contradiction += 1\n",
    "        index +=1\n",
    " \n",
    "    # Creating DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"premise\": premise,\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "    # Define language-specific directory\n",
    "    lang_dir = os.path.join(base_dir, lang)\n",
    "    os.makedirs(lang_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    # Save to CSV\n",
    "    file_path = os.path.join(lang_dir, \"test.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c7e4d59-f437-49da-8fd5-fd30e065dfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved nli_dataset/pol/dev.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "languages = [\"pol\"]\n",
    "\n",
    "base_dir = \"nli_dataset\"\n",
    "\n",
    "for lang in languages:\n",
    "    \n",
    "    # Get the test set\n",
    "    test_data = load_dataset(\"sdadas/sick_pl\")[\"validation\"]\n",
    "    \n",
    "    premise = []\n",
    "    hypothesis = []\n",
    "    label = []\n",
    "    num_entail, num_neutral, num_contradiction = 0,0,0\n",
    "    index = 0\n",
    "    \n",
    "    while (num_entail<10 or num_neutral<10 or num_contradiction<10):\n",
    "        lbl = test_data[index][\"entailment_judgment\"]\n",
    "        if lbl == 'ENTAILMENT' and num_entail<10:\n",
    "            premise.append(test_data[index][\"sentence_A\"])\n",
    "            hypothesis.append(test_data[index][\"sentence_B\"])\n",
    "            label.append(0)\n",
    "            num_entail += 1\n",
    "        if lbl == 'NEUTRAL' and num_neutral<10:\n",
    "            premise.append(test_data[index][\"sentence_A\"])\n",
    "            hypothesis.append(test_data[index][\"sentence_B\"])\n",
    "            label.append(1)\n",
    "            num_neutral += 1\n",
    "        if lbl == 'CONTRADICTION' and num_contradiction<10:\n",
    "            premise.append(test_data[index][\"sentence_A\"])\n",
    "            hypothesis.append(test_data[index][\"sentence_B\"])\n",
    "            label.append(2)\n",
    "            num_contradiction += 1\n",
    "        index +=1\n",
    " \n",
    "    # Creating DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"premise\": premise,\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "    # Define language-specific directory\n",
    "    lang_dir = os.path.join(base_dir, lang)\n",
    "    os.makedirs(lang_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    # Save to CSV\n",
    "    file_path = os.path.join(lang_dir, \"dev.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bb5a560-6738-45dc-8fb4-f41fae1403c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved nli_dataset/pat/test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "languages = [\"pat\"]\n",
    "\n",
    "base_dir = \"nli_dataset\"\n",
    "\n",
    "for lang in languages:\n",
    "    \n",
    "    # Get the test set\n",
    "    ds = load_dataset(\"WillHeld/JamPatoisNLI\")\n",
    "    test_data = ds['test']\n",
    "    val_data = ds['val']\n",
    "    train_data = ds['train']\n",
    "    \n",
    "    premise = []\n",
    "    hypothesis = []\n",
    "    label = []\n",
    "    num_entail, num_neutral, num_contradiction = 0,0,0\n",
    "    index = 0\n",
    "    \n",
    "    while (num_entail<67 or num_neutral<67 or num_contradiction<66):\n",
    "        lbl = train_data[index][\"label\"]\n",
    "        if lbl == 0 and num_entail<67:\n",
    "            premise.append(train_data[index][\"premise\"])\n",
    "            hypothesis.append(train_data[index][\"hypothesis\"])\n",
    "            label.append(0)\n",
    "            num_entail += 1\n",
    "        if lbl == 1 and num_neutral<67:\n",
    "            premise.append(train_data[index][\"premise\"])\n",
    "            hypothesis.append(train_data[index][\"hypothesis\"])\n",
    "            label.append(1)\n",
    "            num_neutral += 1\n",
    "        if lbl == 2 and num_contradiction<66:\n",
    "            premise.append(train_data[index][\"premise\"])\n",
    "            hypothesis.append(train_data[index][\"hypothesis\"])\n",
    "            label.append(2)\n",
    "            num_contradiction += 1\n",
    "        index +=1\n",
    "\n",
    "    for i in range(200):\n",
    "        premise.append(val_data[i][\"premise\"])\n",
    "        hypothesis.append(val_data[i][\"hypothesis\"])\n",
    "        label.append(val_data[i][\"label\"])\n",
    "    for i in range(200):\n",
    "        premise.append(test_data[i][\"premise\"])\n",
    "        hypothesis.append(test_data[i][\"hypothesis\"])\n",
    "        label.append(test_data[i][\"label\"])\n",
    "        \n",
    "    # Creating DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"premise\": premise,\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "    # Define language-specific directory\n",
    "    lang_dir = os.path.join(base_dir, lang)\n",
    "    os.makedirs(lang_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    # Save to CSV\n",
    "    file_path = os.path.join(lang_dir, \"test.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84b89b26-4139-4273-8960-d7b50c75d6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55978d02138e452aab8c1dee585f910f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93d99e2d406444d8d315c014ac0cb54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-11643d27698faf25.parquet:   0%|          | 0.00/24.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e346094296294673b0b2ee9929e0f44d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-94191d14c68fa169.parquet:   0%|          | 0.00/21.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2620a26d104886a36876a786e9aa33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-2ca76346a327770e.parquet:   0%|          | 0.00/21.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0996bc7ad4400083cb7615e12e1f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e024fb3bcc41cdb21c910c1c996d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9e7410fc6949fba4f2ab32f0942ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating val split:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Label Counts: Counter({0: 84, 2: 83, 1: 83})\n",
      "Validation Label Counts: Counter({2: 67, 1: 67, 0: 66})\n",
      "Test Label Counts: Counter({2: 67, 0: 67, 1: 66})\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"WillHeld/JamPatoisNLI\")\n",
    "test_data = ds['test']\n",
    "val_data = ds['val']\n",
    "train_data = ds['train']\n",
    "from collections import Counter\n",
    "\n",
    "# Count labels in each split\n",
    "train_label_counts = Counter(train_data[\"label\"])\n",
    "val_label_counts = Counter(val_data[\"label\"])\n",
    "test_label_counts = Counter(test_data[\"label\"])\n",
    "\n",
    "print(\"Train Label Counts:\", train_label_counts)\n",
    "print(\"Validation Label Counts:\", val_label_counts)\n",
    "print(\"Test Label Counts:\", test_label_counts)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "545bbc6d-91b5-4230-91c8-36895033bcda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbb71af8-387d-4bc2-9006-d7c74ba36e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved nli_dataset/pat/dev.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "languages = [\"pat\"]\n",
    "\n",
    "base_dir = \"nli_dataset\"\n",
    "\n",
    "for lang in languages:\n",
    "    \n",
    "    premise = []\n",
    "    hypothesis = []\n",
    "    label = []\n",
    "    num_entail, num_neutral, num_contradiction = 0,0,0\n",
    "    \n",
    "    while (num_entail<10 or num_neutral<10 or num_contradiction<10):\n",
    "        lbl = train_data[index][\"label\"]\n",
    "        if lbl == 0 and num_entail<10:\n",
    "            premise.append(train_data[index][\"premise\"])\n",
    "            hypothesis.append(train_data[index][\"hypothesis\"])\n",
    "            label.append(0)\n",
    "            num_entail += 1\n",
    "        if lbl == 1 and num_neutral<10:\n",
    "            premise.append(train_data[index][\"premise\"])\n",
    "            hypothesis.append(train_data[index][\"hypothesis\"])\n",
    "            label.append(1)\n",
    "            num_neutral += 1\n",
    "        if lbl == 2 and num_contradiction<10:\n",
    "            premise.append(train_data[index][\"premise\"])\n",
    "            hypothesis.append(train_data[index][\"hypothesis\"])\n",
    "            label.append(2)\n",
    "            num_contradiction += 1\n",
    "        index +=1\n",
    " \n",
    "    # Creating DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"premise\": premise,\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "    # Define language-specific directory\n",
    "    lang_dir = os.path.join(base_dir, lang)\n",
    "    os.makedirs(lang_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    # Save to CSV\n",
    "    file_path = os.path.join(lang_dir, \"dev.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44548a01-b811-4d8a-a9fc-a443c3318aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730cb5f0cb164310969ddaf63bfbf05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/22.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0874a848186242cd9cc271a57119b4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/1.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91bd8de82c24d288b6fe8263bb0d6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/224k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38b3d0a7f654bc28782270ce027c33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/24998 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85480ef988f14043a3248cc92c6fc8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"klue/klue\", \"nli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c2d2038-f043-4b22-828c-df7f3a6ecde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['guid', 'source', 'premise', 'hypothesis', 'label'],\n",
       "        num_rows: 24998\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['guid', 'source', 'premise', 'hypothesis', 'label'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f3a63b3-1db8-4d21-8aee-3e25d713acb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source counts from index 1 to 601: Counter({'wikipedia': 15, 'NSMC': 9, 'airbnb': 6})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Get the 'source' column from index 1 to 601\n",
    "source_counts = Counter(ds['train'][\"source\"][1:31])\n",
    "\n",
    "# Print the counts\n",
    "print(\"Source counts from index 1 to 601:\", source_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1fe70bc0-fe26-49bc-bf23-935c4527b5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source counts from index 1 to 601: Counter({'wikitree': 126, 'airbnb': 117, 'wikinews': 114, 'NSMC': 90, 'wikipedia': 78, 'policy': 75})\n"
     ]
    }
   ],
   "source": [
    "# Get the 'source' column from index 1 to 601\n",
    "source_counts = Counter(ds['validation'][\"source\"][1:601])\n",
    "\n",
    "# Print the counts\n",
    "print(\"Source counts from index 1 to 601:\", source_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "61e356ce-574b-4b51-b806-1cc806b8b2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved nli_dataset/kor/test.csv\n"
     ]
    }
   ],
   "source": [
    "#34 from two categories and 33 from other 4. \n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "languages = [\"kor\"]\n",
    "base_dir = \"nli_dataset\"\n",
    "\n",
    "# Desired category distribution\n",
    "category_limits = {\"wikitree\": 34, \"airbnb\": 34, \"wikinews\": 33, \"NSMC\": 33, \"wikipedia\": 33, \"policy\": 33}\n",
    "counts = {\"wikitree\": 0, \"airbnb\": 0, \"wikinews\": 0, \"NSMC\": 0, \"wikipedia\": 0, \"policy\": 0}\n",
    "for lang in languages:\n",
    "    \n",
    "    # Load dataset\n",
    "    ds = load_dataset(\"klue/klue\", \"nli\")\n",
    "    val_data = ds['validation']\n",
    "\n",
    "    premise = []\n",
    "    hypothesis = []\n",
    "    label = []\n",
    "    index = 1\n",
    "    \n",
    "    while any(counts[cat] < category_limits[cat] for cat in category_limits):        \n",
    "        cat = val_data[index][\"source\"]\n",
    "        if counts[cat] < category_limits[cat]:\n",
    "            for j in range(3): #take in groups of 3. \n",
    "                premise.append(val_data[index+j][\"premise\"])\n",
    "                hypothesis.append(val_data[index+j][\"hypothesis\"])\n",
    "                label.append(val_data[index+j][\"label\"])\n",
    "            counts[cat]+=1\n",
    "        index+=3\n",
    "         \n",
    "    # Creating DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"premise\": premise,\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "    # Define language-specific directory\n",
    "    lang_dir = os.path.join(base_dir, lang)\n",
    "    os.makedirs(lang_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    # Save to CSV\n",
    "    file_path = os.path.join(lang_dir, \"test.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30ee9a8f-2faf-4496-800d-a902c39b390c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved nli_dataset/kor/dev.csv\n"
     ]
    }
   ],
   "source": [
    "#34 from two categories and 33 from other 4. \n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "languages = [\"kor\"]\n",
    "base_dir = \"nli_dataset\"\n",
    "\n",
    "for lang in languages:\n",
    "    \n",
    "    # Load dataset\n",
    "    ds = load_dataset(\"klue/klue\", \"nli\")\n",
    "    val_data = ds['train']\n",
    "\n",
    "    premise = []\n",
    "    hypothesis = []\n",
    "    label = []\n",
    "    # airbnb, wikipedia, wikipedia, nsmc, nsmc, wikinews, wikitree, wikinews,policy\n",
    "    indexes = [4,7,10,19,28,34,43,52,58,61]\n",
    "    \n",
    "    for index in indexes:\n",
    "        for j in range(3): #take in groups of 3. \n",
    "            premise.append(val_data[index+j][\"premise\"])\n",
    "            hypothesis.append(val_data[index+j][\"hypothesis\"])\n",
    "            label.append(val_data[index+j][\"label\"])\n",
    "        index+=1\n",
    "         \n",
    "    # Creating DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"premise\": premise,\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "    # Define language-specific directory\n",
    "    lang_dir = os.path.join(base_dir, lang)\n",
    "    os.makedirs(lang_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    # Save to CSV\n",
    "    file_path = os.path.join(lang_dir, \"dev.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "858051a4-abb4-431c-9b09-95e7913f46d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(premise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f8bec683-8b9f-4504-be19-abd157714630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('nli_dataset/ron/dev.csv')\n",
    "en = df[df['label']==0]\n",
    "p = list(en['premise'])\n",
    "co = df[df['label']==2]\n",
    "p = p + list(en['premise'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "71c53637-a8fc-4091-b51c-fe57c8823d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p + premise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7ecbc4d9-79ff-459b-8d3f-f102f452d3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "05a519c1-28ce-4258-b9e0-33cba60580f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1_list = [entry[\"sentence1\"] for entry in val_data if entry[\"label\"] in [0, 1]]\n",
    "len(sentence1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b38e9b00-490a-42a7-ad78-61e4f9034c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved nli_dataset/ron/dev.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# take 10, entail con neu from val set to dev.csv, take remaining con, en to test.csv. From test set take all con, ent and remaining neutral to make 600\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import json \n",
    "\n",
    "languages = [\"ron\"]\n",
    "\n",
    "base_dir = \"nli_dataset\"\n",
    "\n",
    "for lang in languages:\n",
    "    \n",
    "    with open('test.json', 'r') as file:\n",
    "        test_data = json.load(file)    \n",
    "    with open('validation.json', 'r') as file:\n",
    "        val_data = json.load(file)      \n",
    "        \n",
    "    premise = []\n",
    "    hypothesis = []\n",
    "    label = []\n",
    "    premise_test = []\n",
    "    hypothesis_test = []\n",
    "    label_test = []\n",
    "    num_entail, num_neutral, num_contradiction = 0,0,0\n",
    "    index = 0\n",
    "    \n",
    "    while (num_entail<10 or num_neutral<10 or num_contradiction<10):\n",
    "        lbl = val_data[index][\"label\"]\n",
    "        if lbl == 1: \n",
    "            if num_entail<10:\n",
    "                premise.append(val_data[index][\"sentence1\"])\n",
    "                hypothesis.append(val_data[index][\"sentence2\"])\n",
    "                label.append(0)\n",
    "                num_entail += 1\n",
    "            else: \n",
    "                premise_test.append(val_data[index][\"sentence1\"])\n",
    "                hypothesis_test.append(val_data[index][\"sentence2\"])\n",
    "                label_test.append(0)\n",
    "        if lbl == 3 and num_neutral<10:\n",
    "            premise.append(val_data[index][\"sentence1\"])\n",
    "            hypothesis.append(val_data[index][\"sentence2\"])\n",
    "            label.append(1)\n",
    "            num_neutral += 1\n",
    "        if lbl == 0:\n",
    "            if num_contradiction<10:\n",
    "                premise.append(val_data[index][\"sentence1\"])\n",
    "                hypothesis.append(val_data[index][\"sentence2\"])\n",
    "                label.append(2)\n",
    "                num_contradiction += 1\n",
    "            else:\n",
    "                premise_test.append(val_data[index][\"sentence1\"])\n",
    "                hypothesis_test.append(val_data[index][\"sentence2\"])\n",
    "                label_test.append(2)\n",
    "        index +=1\n",
    " \n",
    "    # Creating DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"premise\": premise,\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "    # Define language-specific directory\n",
    "    lang_dir = os.path.join(base_dir, lang)\n",
    "    os.makedirs(lang_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    # Save to CSV\n",
    "    file_path = os.path.join(lang_dir, \"dev.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved {file_path}\")\n",
    "    \n",
    "    while (index<len(val_data)):\n",
    "        lbl = val_data[index][\"label\"]\n",
    "        if lbl == 1:\n",
    "            premise_test.append(val_data[index][\"sentence1\"])\n",
    "            hypothesis_test.append(val_data[index][\"sentence2\"])\n",
    "            label_test.append(0)\n",
    "        if lbl == 0:\n",
    "            premise_test.append(val_data[index][\"sentence1\"])\n",
    "            hypothesis_test.append(val_data[index][\"sentence2\"])\n",
    "            label_test.append(2)\n",
    "        index +=1\n",
    "\n",
    "num_entail, num_neutral, num_contradiction =0,0,0\n",
    "index = 0\n",
    "\n",
    "while (num_entail<96 or num_neutral<304 or num_contradiction<74):\n",
    "    lbl = test_data[index][\"label\"]\n",
    "    if lbl == 1 and num_entail<96:\n",
    "        premise_test.append(test_data[index][\"sentence1\"])\n",
    "        hypothesis_test.append(test_data[index][\"sentence2\"])\n",
    "        label_test.append(0)\n",
    "        num_entail += 1\n",
    "    if lbl == 3 and num_neutral<304:\n",
    "        premise_test.append(test_data[index][\"sentence1\"])\n",
    "        hypothesis_test.append(test_data[index][\"sentence2\"])\n",
    "        label_test.append(1)\n",
    "        num_neutral += 1\n",
    "    if lbl == 0 and num_contradiction<74:\n",
    "        premise_test.append(test_data[index][\"sentence1\"])\n",
    "        hypothesis_test.append(test_data[index][\"sentence2\"])\n",
    "        label_test.append(2)\n",
    "        num_contradiction += 1\n",
    "    index +=1\n",
    "# Creating DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"premise\": premise_test,\n",
    "    \"hypothesis\": hypothesis_test,\n",
    "    \"label\": label_test\n",
    "})\n",
    "\n",
    "# Define language-specific directory\n",
    "lang_dir = os.path.join(base_dir, lang)\n",
    "os.makedirs(lang_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "# Save to CSV\n",
    "file_path = os.path.join(lang_dir, \"test.csv\")\n",
    "df.to_csv(file_path, index=False)\n",
    "print(f\"Saved {file_path}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f57b521-f9a9-4597-8cae-4e2cc8f0a75e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma3-env",
   "language": "python",
   "name": "gemma3-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
