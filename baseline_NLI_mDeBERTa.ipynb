{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5daad3a-157d-4f92-8705-72162ee9fbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Load MNLI dataset\n",
    "mnli = load_dataset(\"nyu-mll/multi_nli\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"microsoft/mdeberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "# Preprocess\n",
    "def preprocess(example):\n",
    "    return tokenizer(example[\"premise\"], example[\"hypothesis\"], truncation=True)\n",
    "\n",
    "encoded = mnli.map(preprocess, batched=True)\n",
    "encoded = encoded.rename_column(\"label\", \"labels\")\n",
    "encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Evaluation metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mdeberta-mnli\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # if using GPU with mixed precision\n",
    "    report_to=\"none\",  # disable wandb\n",
    ")\n",
    "\n",
    "# Data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded[\"train\"],\n",
    "    eval_dataset=encoded[\"validation_matched\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train!\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"mdeberta-v3-mnli-finetuned\")\n",
    "tokenizer.save_pretrained(\"mdeberta-v3-mnli-finetuned\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec974b-4439-4df6-bd01-c799e85d4906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"mdeberta-v3-mnli-finetuned\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mdeberta-v3-mnli-finetuned\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03a96f63-7da8-4156-bc2d-60063da96b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "parent_folder = 'nli_dataset'\n",
    "language_codes = [name for name in os.listdir(parent_folder)\n",
    "                if os.path.isdir(os.path.join(parent_folder, name))]\n",
    "language_codes.sort()\n",
    "languages_to_run = language_codes\n",
    "\n",
    "from tqdm import tqdm\n",
    "labels = {}\n",
    "result_accuracies = {}\n",
    "\n",
    "for language_code in languages_to_run:\n",
    "    print(language_code)\n",
    "    labels[language_code]=[]\n",
    "    result_accuracies[language_code] = []\n",
    "    accurate = 0\n",
    "    \n",
    "    df = pd.read_csv(\"nli_dataset/{}/test.csv\".format(language_code))\n",
    "    \n",
    "    for i in tqdm(range(600)): #length of devtest\n",
    "        premise = df.iloc[i]['premise']\n",
    "        hypothesis = df.iloc[i]['hypothesis']\n",
    "        gold_label = df.iloc[i]['label']\n",
    "        \n",
    "        inputs = tokenizer(premise, hypothesis, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            pred = torch.argmax(logits, dim=-1).item()\n",
    "            \n",
    "        labels[language_code].append(pred)\n",
    "        if pred == gold_label:\n",
    "            accurate+=1\n",
    "            \n",
    "    result_accuracies[language_code] = accurate\n",
    "    \n",
    "    result_df = pd.DataFrame({\n",
    "    \"premise\": df['premise'],\n",
    "    \"hypothesis\": df['hypothesis'],\n",
    "    \"gpt_label\": labels[language_code]\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    result_df.to_csv(\"nli_predicted_labels_mdeberta/{}.csv\".format(language_code), index=False)\n",
    "    print(accurate)\n",
    "\n",
    "print(result_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b415633f-35fe-46c1-a1cd-bce1d8d0ac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_accuracies = {'amh': 442, 'ara': 463, 'asm': 418, 'aym': 243, 'ben': 463, 'bul': 480, 'bzd': 265, 'cat': 488, 'cni': 255, 'deu': 482, 'ell': 486, 'eng': 530, 'ewe': 231, 'fra': 504, 'grn': 260, 'guj': 438, 'hau': 414, 'hch': 221, 'hin': 468, 'ibo': 403, 'ind': 453, 'jpn': 483, 'kan': 467, 'kin': 368, 'kor': 474, 'lin': 204, 'lug': 284, 'mal': 450, 'mar': 430, 'mya': 460, 'nah': 254, 'ori': 369, 'orm': 300, 'oto': 258, 'pan': 451, 'pat': 362, 'pol': 459, 'por': 542, 'quy': 266, 'ron': 320, 'rus': 473, 'shp': 281, 'sna': 382, 'sot': 358, 'spa': 495, 'swa': 418, 'tam': 456, 'tar': 220, 'tel': 443, 'tha': 446, 'tur': 463, 'twi': 276, 'urd': 418, 'vie': 474, 'wol': 246, 'xho': 390, 'yor': 256, 'zho': 473, 'zul': 398}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc476e6-bec8-4ffb-9eed-174dd238a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# import torch\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"mdeberta-v3-mnli-finetuned\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mdeberta-v3-mnli-finetuned\")\n",
    "# model.eval()\n",
    "# # Your example\n",
    "# premise = \"The cat sat on the mat.\"\n",
    "# hypothesis = \"A cat is sitting.\"\n",
    "\n",
    "# inputs = tokenizer(premise, hypothesis, return_tensors=\"pt\", truncation=True)\n",
    "# with torch.no_grad():\n",
    "#     logits = model(**inputs).logits\n",
    "#     pred = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "# label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "# print(f\"Prediction: {label_map[pred]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c93dd6b7-5301-4c97-9adf-97d30a86ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_code_to_name = {\n",
    "    'amh': 'Amharic',\n",
    "    'ara': 'Arabic',\n",
    "    'asm': 'Assamese',\n",
    "    'aym': 'Aymara',\n",
    "    'ben': 'Bengali',\n",
    "    'bul': 'Bulgarian',\n",
    "    'bzd': 'Bribri',\n",
    "    'cat': 'Catalan',\n",
    "    'cni': 'Asháninka',\n",
    "    'deu': 'German',\n",
    "    'ell': 'Greek',\n",
    "    'eng': 'English',\n",
    "    'ewe': 'Ewe',\n",
    "    'fra': 'French',\n",
    "    'grn': 'Guarani',\n",
    "    'guj': 'Gujarati',\n",
    "    'hau': 'Hausa',\n",
    "    'hch': 'Wixarika',\n",
    "    'hin': 'Hindi',\n",
    "    'ibo': 'Igbo',\n",
    "    'ind': 'Indonesian',\n",
    "    'jpn': 'Japanese',\n",
    "    'kan': 'Kannada',\n",
    "    'kin': 'Kinyarwanda',\n",
    "    'kor': 'Korean',\n",
    "    'lin': 'Lingala',\n",
    "    'lug': 'Luganda',\n",
    "    'mal': 'Malayalam',\n",
    "    'mar': 'Marathi',\n",
    "    'mya': 'Burmese',\n",
    "    'nah': 'Nahuatl',\n",
    "    'ori': 'Odia (Oriya)',\n",
    "    'orm': 'Oromo',\n",
    "    'oto': 'Otomi',\n",
    "    'pan': 'Punjabi',\n",
    "    'pat': 'Jamaican Patois',\n",
    "    'pol': 'Polish',\n",
    "    'por': 'Portuguese',\n",
    "    'quy': 'Quechua',\n",
    "    'ron': 'Romanian',\n",
    "    'rus': 'Russian',\n",
    "    'shp': 'Shipibo-Conibo',\n",
    "    'sna': 'chiShona',\n",
    "    'sot': 'Sesotho',\n",
    "    'spa': 'Spanish',\n",
    "    'swa': 'Swahili',\n",
    "    'tam': 'Tamil',\n",
    "    'tar': 'Rarámuri',\n",
    "    'tel': 'Telugu',\n",
    "    'tha': 'Thai',\n",
    "    'tur': 'Turkish',\n",
    "    'twi': 'Twi',\n",
    "    'urd': 'Urdu',\n",
    "    'vie': 'Vietnamese',\n",
    "    'wol': 'Wolof',\n",
    "    'xho': 'isiXhosa',\n",
    "    'yor': 'Yoruba',\n",
    "    'zho': 'Chinese',\n",
    "    'zul': 'isiZulu'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fd8f606-635c-44c5-8ad5-81d3c83bb59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    \"Language name\": [lang_code_to_name[language_code] for language_code in language_codes],\n",
    "    \"Language code\": language_codes,\n",
    "    \"Accuracy\": [round(result_accuracies[language_code]*100/600, 1) for language_code in language_codes]\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"nli_results_mDeBERTa.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0403ec-1bcf-4d5d-9e3d-15aa26c6ff46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma3-env",
   "language": "python",
   "name": "gemma3-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
