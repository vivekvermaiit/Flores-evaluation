{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2757e8-94c2-4b99-868b-c90eb617d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='0'\n",
    "\n",
    "# Load MNLI dataset\n",
    "mnli = load_dataset(\"nyu-mll/multi_nli\")\n",
    "#mnli[\"train\"] = mnli[\"train\"].select(range(100))\n",
    "#mnli[\"validation_matched\"] = mnli[\"validation_matched\"].select(range(100))\n",
    "model_name = \"fdschmidt93/NLLB-LLM2Vec-Meta-Llama-31-8B-Instruct-mntp-unsup-simcse\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "\n",
    "    num_labels=3, trust_remote_code=True).to(device)\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout = 0.05,\n",
    "    target_modules=r\".*llm2vec.*(self_attn\\.(q|k|v|o)_proj|mlp\\.(gate|up|down)_proj).*\",\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ed596e-ff4a-4a31-8ac5-79817fabb9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "def preprocess(example):\n",
    "    return tokenizer(example[\"premise\"], example[\"hypothesis\"], truncation=True)\n",
    "\n",
    "encoded = mnli.map(preprocess, batched=True)\n",
    "encoded = encoded.rename_column(\"label\", \"labels\")\n",
    "encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Evaluation metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    if isinstance(logits, tuple):  # handle case where logits is a tuple\n",
    "        logits = logits[0]\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "# Dataset size and batch size\n",
    "train_batch_size = 8\n",
    "num_train_samples = len(encoded[\"train\"])\n",
    "steps_per_epoch = ceil(num_train_samples / train_batch_size)\n",
    "eval_steps = steps_per_epoch // 5  # 20% of an epoch\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./nllb-mnli\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    save_steps = eval_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=train_batch_size,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # if using GPU with mixed precision\n",
    "    report_to=\"none\",  # disable wandb\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=eval_steps,\n",
    ")\n",
    "\n",
    "# Data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded[\"train\"],\n",
    "    eval_dataset=encoded[\"validation_matched\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train!\n",
    "trainer.train()\n",
    "model.save_pretrained(\"nllbllm2vec-mnli-finetuned\")\n",
    "tokenizer.save_pretrained(\"nllbllm2vec-mnli-finetuned\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc2dfb-f466-44ad-9562-ee2d07f86766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# import torch\n",
    "# from transformers import BitsAndBytesConfig\n",
    "# import os\n",
    "# from datasets import Dataset\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] ='0'\n",
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "# )\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     \"nllbllm2vec-mnli-finetuned\",\n",
    "#     quantization_config=bnb_config,\n",
    "#     num_labels=3, trust_remote_code=True).to(device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"nllbllm2vec-mnli-finetuned\", trust_remote_code=True)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511f8b19-9548-4d50-b83c-e66e519b6e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# from datasets import Dataset\n",
    "# from torch.utils.data import DataLoader\n",
    "# from transformers import DataCollatorWithPadding\n",
    "# from transformers import Trainer\n",
    "# trainer = Trainer(model=model)  # Reload trainer with the model\n",
    "\n",
    "# parent_folder = 'nli_dataset'\n",
    "# language_codes = [name for name in os.listdir(parent_folder)\n",
    "#                 if os.path.isdir(os.path.join(parent_folder, name))]\n",
    "# language_codes.sort()\n",
    "# languages_to_run = language_codes\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# # labels = {}\n",
    "# result_accuracies = {}\n",
    "# def preprocess(example):\n",
    "#     return tokenizer(example[\"premise\"], example[\"hypothesis\"], truncation=True)\n",
    "\n",
    "\n",
    "# for language_code in languages_to_run:\n",
    "#     print(language_code)\n",
    "#     # labels[language_code]=[]\n",
    "#     result_accuracies[language_code] = []\n",
    "    \n",
    "#     df = pd.read_csv(\"nli_dataset/{}/test.csv\".format(language_code))\n",
    "#     dataset = Dataset.from_pandas(df)\n",
    "#     # Tokenize\n",
    "#     dataset = dataset.map(preprocess, batched=True)\n",
    "#     dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "#     dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "#     # fill in the blank here. \n",
    "#     predictions_output = trainer.predict(dataset)\n",
    "#     logits = predictions_output.predictions\n",
    "#     preds = np.argmax(logits, axis=-1)\n",
    "#     labels = predictions_output.label_ids\n",
    "#     accurate = (preds == labels)\n",
    "        \n",
    "#     result_accuracies[language_code] = accurate\n",
    "    \n",
    "#     result_df = pd.DataFrame({\n",
    "#     \"premise\": df['premise'],\n",
    "#     \"hypothesis\": df['hypothesis'],\n",
    "#     \"gpt_label\": preds\n",
    "#     })\n",
    "\n",
    "#     # Save to CSV\n",
    "#     result_df.to_csv(\"nli_predicted_labels_nllbllm2vec/{}.csv\".format(language_code), index=False)\n",
    "#     print(accurate)\n",
    "\n",
    "# print(result_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3989fdf2-39bf-407f-9243-804824e9be70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "parent_folder = 'nli_dataset'\n",
    "language_codes = [name for name in os.listdir(parent_folder)\n",
    "                if os.path.isdir(os.path.join(parent_folder, name))]\n",
    "language_codes.sort()\n",
    "languages_to_run = language_codes\n",
    "\n",
    "from tqdm import tqdm\n",
    "# labels = {}\n",
    "result_accuracies = {}\n",
    "\n",
    "for language_code in languages_to_run:\n",
    "    print(language_code)\n",
    "    # labels[language_code]=[]\n",
    "    result_accuracies[language_code] = []\n",
    "\n",
    "    # Load and prepare test dataset\n",
    "    df = pd.read_csv(\"nli_dataset/{}/test.csv\".format(language_code))\n",
    "    test_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    # Preprocess\n",
    "    test_dataset = test_dataset.map(preprocess, batched=True)\n",
    "    test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "    test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    # Run evaluation\n",
    "    pred_output = trainer.predict(test_dataset)\n",
    "    preds = np.argmax(pred_output.predictions[0], axis=-1)\n",
    "    df[\"predicted_label\"] = preds\n",
    "    df[\"correct\"] = (df[\"label\"] == df[\"predicted_label\"]).astype(int)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accurate = df[\"correct\"].sum()\n",
    "\n",
    "\n",
    "    result_accuracies[language_code] = accurate\n",
    "\n",
    "    result_df = pd.DataFrame({\n",
    "    \"premise\": df['premise'],\n",
    "    \"hypothesis\": df['hypothesis'],\n",
    "    \"gpt_label\": preds\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    result_df.to_csv(\"nli_predicted_labels_nllbllm2vec/{}.csv\".format(language_code), index=False)\n",
    "    print(accurate)\n",
    "\n",
    "print(result_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e2a7608-58e0-42e6-849d-e4448ce1dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "result_accuracies = {'amh': np.int64(405), 'ara': np.int64(427), 'asm': np.int64(402), 'aym': np.int64(278), 'ben': np.int64(448), 'bul': np.int64(472), 'bzd': np.int64(273), 'cat': np.int64(465), 'cni': np.int64(262), 'deu': np.int64(457), 'ell': np.int64(476), 'eng': np.int64(562), 'ewe': np.int64(270), 'fra': np.int64(509), 'grn': np.int64(290), 'guj': np.int64(444), 'hau': np.int64(302), 'hch': np.int64(239), 'hin': np.int64(448), 'ibo': np.int64(362), 'ind': np.int64(446), 'jpn': np.int64(385), 'kan': np.int64(441), 'kin': np.int64(270), 'kor': np.int64(388), 'lin': np.int64(218), 'lug': np.int64(273), 'mal': np.int64(421), 'mar': np.int64(412), 'mya': np.int64(428), 'nah': np.int64(257), 'ori': np.int64(440), 'orm': np.int64(261), 'oto': np.int64(245), 'pan': np.int64(447), 'pat': np.int64(425), 'pol': np.int64(335), 'por': np.int64(496), 'quy': np.int64(287), 'ron': np.int64(284), 'rus': np.int64(478), 'shp': np.int64(277), 'sna': np.int64(301), 'sot': np.int64(313), 'spa': np.int64(519), 'swa': np.int64(338), 'tam': np.int64(434), 'tar': np.int64(245), 'tel': np.int64(425), 'tha': np.int64(425), 'tur': np.int64(358), 'twi': np.int64(267), 'urd': np.int64(425), 'vie': np.int64(463), 'wol': np.int64(232), 'xho': np.int64(329), 'yor': np.int64(284), 'zho': np.int64(463), 'zul': np.int64(315)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c767114f-ecf4-42a7-be27-1ee81160c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_code_to_name = {\n",
    "    'amh': 'Amharic',\n",
    "    'ara': 'Arabic',\n",
    "    'asm': 'Assamese',\n",
    "    'aym': 'Aymara',\n",
    "    'ben': 'Bengali',\n",
    "    'bul': 'Bulgarian',\n",
    "    'bzd': 'Bribri',\n",
    "    'cat': 'Catalan',\n",
    "    'cni': 'Asháninka',\n",
    "    'deu': 'German',\n",
    "    'ell': 'Greek',\n",
    "    'eng': 'English',\n",
    "    'ewe': 'Ewe',\n",
    "    'fra': 'French',\n",
    "    'grn': 'Guarani',\n",
    "    'guj': 'Gujarati',\n",
    "    'hau': 'Hausa',\n",
    "    'hch': 'Wixarika',\n",
    "    'hin': 'Hindi',\n",
    "    'ibo': 'Igbo',\n",
    "    'ind': 'Indonesian',\n",
    "    'jpn': 'Japanese',\n",
    "    'kan': 'Kannada',\n",
    "    'kin': 'Kinyarwanda',\n",
    "    'kor': 'Korean',\n",
    "    'lin': 'Lingala',\n",
    "    'lug': 'Luganda',\n",
    "    'mal': 'Malayalam',\n",
    "    'mar': 'Marathi',\n",
    "    'mya': 'Burmese',\n",
    "    'nah': 'Nahuatl',\n",
    "    'ori': 'Odia (Oriya)',\n",
    "    'orm': 'Oromo',\n",
    "    'oto': 'Otomi',\n",
    "    'pan': 'Punjabi',\n",
    "    'pat': 'Jamaican Patois',\n",
    "    'pol': 'Polish',\n",
    "    'por': 'Portuguese',\n",
    "    'quy': 'Quechua',\n",
    "    'ron': 'Romanian',\n",
    "    'rus': 'Russian',\n",
    "    'shp': 'Shipibo-Conibo',\n",
    "    'sna': 'chiShona',\n",
    "    'sot': 'Sesotho',\n",
    "    'spa': 'Spanish',\n",
    "    'swa': 'Swahili',\n",
    "    'tam': 'Tamil',\n",
    "    'tar': 'Rarámuri',\n",
    "    'tel': 'Telugu',\n",
    "    'tha': 'Thai',\n",
    "    'tur': 'Turkish',\n",
    "    'twi': 'Twi',\n",
    "    'urd': 'Urdu',\n",
    "    'vie': 'Vietnamese',\n",
    "    'wol': 'Wolof',\n",
    "    'xho': 'isiXhosa',\n",
    "    'yor': 'Yoruba',\n",
    "    'zho': 'Chinese',\n",
    "    'zul': 'isiZulu'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "deefd752-485a-4b09-8788-985083c8c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "parent_folder = 'nli_dataset'\n",
    "language_codes = [name for name in os.listdir(parent_folder)\n",
    "                if os.path.isdir(os.path.join(parent_folder, name))]\n",
    "language_codes.sort()\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    \"Language name\": [lang_code_to_name[language_code] for language_code in language_codes],\n",
    "    \"Language code\": language_codes,\n",
    "    \"Accuracy\": [round(result_accuracies[language_code]*100/600, 1) for language_code in language_codes]\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"nli_results_nllbllm2vec.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a99536-e66e-4b99-a599-d0e099298b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma3-env",
   "language": "python",
   "name": "gemma3-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
